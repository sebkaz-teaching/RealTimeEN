---
layout: page
title: Lecture 2 - Time to Stream
mathjax: true
---


## Batch vs Stream Processing

Expectations vs Reality

<img alt="Batch Processing" src="../img/batch00.png" align="center" />


When to take a business decision ? 


<img alt="Batch Processing" src="../img/batch0.png" align="center" />


### Rodzaj danych

1. Batch = Big, historical datasets
2. Stream = stream data, on line, generated and send continuously

### Czas uruchomienia przetwarzania

1. Batch = minutes, houres, days (Data warehouses)
2. Stream = Real-time/near-real-time

### Ponowne przetwarzanie

1. Batch = possible and used very often
2. Stream = ,,impossible''


<img alt="Batch Processing" src="../img/batch1.png" align="center" />


### ETL

Extract, Transform, Load is a basic pattern for data processing,
commonly known in data warehousing. It's all about *extracting* data from
a source, *transforming* the data (business rules) and at the end *writing/loading*
everything to a target (Hadoop, Relational Database, Data Warehouse etc.)

## Big Data

Big Data system can be a part of (source of) data warehouses (ex. Data Lake, Enterprise Data Hub)

But Data Warehouses are not Big Data Systems!

1. Data warehouses
- highly structured data retention
- focused on the analysis and reporting process
- 100 \% accuracy

2. Big Data
- data of any structure
- serves a variety of data-driven purposes (analytics, data science ...)
- less than 100 \% accuracy


<img alt="Batch Processing" src="../img/batch2.png" align="center" />


## Hadoop Map Reduce

<img alt="Batch Processing" src="../img/batch3.png" align="center" />

> Find a simple map reduce algorithm in any programming language and run it.

<img alt="Batch Processing" src="../img/batch4.png" align="center" />

How to improve ?

### APACHE SPARK

<img alt="Batch Processing" src="../img/batch5.png" align="center" />

<br/>
<img alt="Batch Processing" src="../img/batch6.png" align="center" />



## Strumienie Danych


> Check Your knowladge with [streaming data](https://medium.com/cuelogic-technologies/analyzing-data-streaming-using-spark-vs-kafka-bcfdc33ac828)

> Definition - *Event* is everything that we can observe at a certain moment in time (pure physics).
> Define - In the case of data, *event* is understood as **unchangeable** record in the data stream encoded as JSON, XML, CSV or binary.

Which of the following statements do not represent events?
1. It was a warm day.
2. The API client did not work.
3. The API client stopped working at midnight.
4. The car was black.
5. Murzynek was tasty.
6. In 2013, the NASDAQ exchange opened at 9:30 am every day.

> Definition - A continuous stream of events is an infinite set of individual events arranged in time, e.g. logs from a device.


Systems generating continuous streams of events:
- Transaction systems - e.g. purchase / sale
- Data warehouses - stores the history of events for later analysis (Fact Table).
- Data from sensors: Monitoring systems, IoT, any API (Twitter), queuing systems (Kafka),
- Location tracking
- User interaction: what are your site visitors doing?
- Mobile devices
- Cloud applications
- web browsers


<img alt="Batch Processing" src="../img/stream1.png" align="center" />

- Webserver logs
- Recommendation systems (personalization)

Streaming analytics allows you to quickly catch trends and stay ahead of the competition that uses only batch processing. Reacting to sudden events on the stock market or in social networks in order to prevent or minimize losses.

- which vehicle of the company's fleet is almost empty and - where to send the driver for refueling.
- Which vehicle in your fleet consumes the most fuel and why?
- Which equipment in the plant or the factory may fail in the coming days?
- What spare parts will you need to replace and on which machines in the near future?
- How many customers are currently shopping in the ~ store and ~ can you offer them something?
- Is the customer calling to terminate the contract?
- and many others.

**An enterprise is an organization that generates and responds to a continuous stream of events**.


### Data Streams

> Definition - Data stream is data created incrementally over time, generated from static data (database, reading lines from a file) or dynamically (logs, sensors, functions).

Streaming analytics is also called event stream processing - processing large amounts of data already at the stage of their generation.

They are generated as a direct result of an action.

Regardless of the technology used, all data is created as a continuous stream of events (user actions on the website, system logs, measurements from sensors).

> Find a process that creates an entire data table at once?

The application that processes the stream of events should enable the processing and saving of the event and access (at the same time) to other data so that it can process the event (perform any conversion on it) and save it as a local state.
This state can be saved in many places, eg program variables, local files, ext, and ext of the database. One of the most famous applications of this type is Apache Kafka, which can be combined with e.g. Apache Spark or Apache Flink.

> Find info on how log processing systems work.

These types of applications are used in three classes of applications:

1. event-driven applications
2. data pipeline applications
3. data analytics applications

> Real implementations most often use several classes in one application.

### Event-driven apps
All Event-driven applications process data as Stateful stream processing, processing events with a certain set (mostly business) logic. These applications can trigger actions in response to the analyzed events, e.g. send an alarm in the form of a text message or email, save and forward the appropriate event (e.g. fraud) to the output application (consumer).

Standard applications for this type of application are:

1. Real-time recommendations - e.g. while browsing websites
2. Pattern detection or complex analysis, eg fraud detection in credit card transactions.
3. Anomaly detection - detection of intrusions in a computer network

Applications of this type are treated as a natural evolution of microservices. They communicate with each other using REST requests (be sure to check where you are using them !!!) with the use of straightforward data. They also use data in the form of JSON.

> What is REST?


Log communication (usually asynchronous) distinguishes the sending application (producer) and the consuming application (consumer).

> What is asynchronous communication?

### Data Pipelines

Dane generowane przez firmy najczęściej przechowywane są w wielu różnych formatach i strukturach danych, co związane jest z wykorzystaniem różnych systemów
np. relacyjne i nierelacyjne bazy danych, logowanie zdarzeń, systemy plików (np Hadoop), przechowywanie w pamięci etc. Zapis tych samych danych w różnych systemach zwiększa ich dostępność i łatwość przetwarzania.

Jednak takie podejście wymaga aby systemy te były ze sobą zawsze zsynchronizowane (zawierały te same dane).

Tradycyjne podejście synchronizacji danych to wykonywanie okresowo zadań ETL. Jendak bardzo często nie spełniają one założeń wydajnościowych i jakościowych (czas przetwarzania). Alternatywnie moża użyć informacji rozsyłanych w event logach.
W tej sytuacji agregacje czy normalizacja danych mogą być przeprowadzane dopiero podczas przetwarzania danego zdarzenia.

Tego typu aplikacje, które pozwalają osiągnąć bardzo dużą wydajność (niską latencje) nazywamy `data pipelines`.
W ogólności to aplikacje czytające dużo danych z różnych aplikacji wejściowych i przetwarzających wszystko w bardzo krótkim czasie (np Kafka, Apache Flink).


### Data analytics applications (streaming analytics)

Zadania ETL okresowo (i periodycznie w czasie) importują dane do bazy danych, przy czym dane te są przetwarzane przez zaplanowane zapytania.
Niezależnie od architektury hurtowni danych (lub składników np. Hadoop) przetwarzanie to jest przetwarzaniem `batchowym`.
Procesy analityczne (OLAP) wykonywane są na hurtowni, która uzupełniana jest w powyżej opisany sposób. Mimo, iż periodyczne uzupełnianie danych (czyli cały proces ETL) to swego rodzaju `sztuka` to wprowadza ona w proces analityczne spore opóźnienia.
W zależności od harmonogramu dogrywania danych kolejny punkt danych może pojawić się po godzinie lub nawet kilku dniach.
Nawet w przypadku ciągłego uzupełniania hurtowni (data pipeline app) wciąż pojawia się znaczne opóźnienie w analitycznym odpytywaniu takiej bazy.
O ile takie opóźnienia były akceptowalne w przeszłości, dzisiejsze aplikacje muszą zbierać, przetwarzać i analizować dane w czasie rzeczywistym (personalizowanie zachowań użytkownika na stronach www czy dynamiczna zmiana warunków w grach mobilnych).

Zamiast czekać na okresowe wyzwalanie, aplikacja do analizy strumieniowej w sposób ciągły pobiera strumienie zdarzeń i aktualizuje wyniki, włączając najnowsze zdarzenia z małym opóźnieniem.
Aplikacje do przesyłania strumieniowego przechowują wyniki w zewnętrznym magazynie danych, który obsługuje wydajne aktualizacje (wydajna baza danych lub baza oparta o klucz-wartość).
Odświerzane na żywo wyniki działania aplikacji przesyłania strumieniowego mogą zasilać różnego rodzaju wykresy (prezentowane na żywo).

 <img alt="OLAP system" src="../img/baza3.png" align="center" />

Strumieniowe Aplikacje Analityczne używane są w:
1. Monitorowanie jakości sieci telefonicznych
2. Analiza behawioralna uzytkowników aplikacji mobilnych
3. ...
